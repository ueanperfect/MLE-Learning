$$L = (1-Y) * \frac{1}{2} D^2 + Y * \frac{1}{2} max(m-D, 0)^2$$

# 分类损失函数
## Focal loss
Focal Loss的主要思想是降低易分类的样本的权重，使得难分类的样本能够得到更多的关注和权重。该损失函数引入了一个可调参数$\gamma$，用于控制易分类样本和难分类样本的权重比例。当$\gamma=0$时，Focal Loss退化为普通的交叉熵损失函数。
Focal Loss的数学表达式为：
$FL(p_t) = -(1-p_t)^\gamma \cdot \log(p_t)$
其中，$p_t$是模型预测的概率值，$\gamma$是可调参数，$-\log(p_t)$是交叉熵损失函数。
当样本被正确分类时，$p_t$趋近于1，因此损失函数趋近于0；当样本被错误分类时，$p_t$趋近于0，但是由于引入了$(1-p_t)^\gamma$的权重，因此损失函数仍然可以保持一定的值。这样，Focal Loss可以让模型更加关注难分类的样本，从而提高模型在少数类别上的表现。

## cross entropy loss
交叉熵损失函数（Cross-Entropy Loss）是用于分类任务中的一种损失函数，通常用于训练神经网络模型。交叉熵损失函数衡量了模型预测的类别概率分布与真实类别概率分布之间的差距，从而用于优化模型参数。
假设有一个样本$x$，其真实标签为$y$，模型预测的类别概率分布为$p=(p_1,p_2,\cdots,p_C)$，其中$C$是类别数量。则交叉熵损失函数的数学表达式为：
$L=-\sum_{i=1}^{C} y_i\log(p_i)$
其中，$y_i$是一个二元变量，表示样本$x$的真实标签是否属于第$i$类。当$y_i=0$时，相应的一项会被忽略，当$y_i=1$时，相应的一项会被保留。因此，交叉熵损失函数的值越小，表示模型的预测结果与真实标签越接近。
在训练过程中，交叉熵损失函数通常与反向传播算法一起使用，通过计算梯度来更新神经网络的参数。该损失函数在许多分类任务中得到了广泛应用，例如图像分类、自然语言处理等领域。

二分类的交叉商损失函数可以被看成是多分类的交叉商损失函数的一种拓展。

# 对比学习损失函数
## Cosine Similarity Loss
余弦相似度损失（Cosine Similarity Loss）：余弦相似度损失通过计算样本对之间的余弦相似度来衡量样本之间的相似性，将余弦相似度的值域从 [-1, 1] 映射到 [0, 1]，并通过最大化相似样本之间的余弦相似度，最小化不相似样本之间的余弦相似度，来优化特征表示。
## Contrastive Loss
对比损失（Contrastive Loss）是对比学习中常用的一种损失函数，其主要用于学习样本对之间的相似性和差异性，优化模型的特征表示能力。
对于给定的一对样本 $(x_i, x_j)$，对比损失的计算方式如下：
其中，$d$ 表示样本对之间的距离，$y$ 表示样本对是否相似，$margin$ 表示样本对之间的距离阈值，控制相似样本之间的距离不超过一定范围，$L_{contrastive}$ 表示对比损失。
具体来说，当样本对之间相似时，即 $y=1$ 时，对比损失主要考虑将它们的距离 $d$ 缩小到一定的范围内，从而提高它们之间的相似性；当样本对之间不相似时，即 $y=0$ 时，对比损失主要考虑增大它们的距离 $d$，使它们之间的差异性更加明显，从而提高模型的泛化能力。
对比损失通常与另一个损失函数一起使用，如交叉熵损失或 softmax 损失，从而实现更好的分类效果。在深度学习中，对比损失通常可以通过反向传播算法来优化模型参数，提高特征表示能力。

## Euclidean Distance Loss
欧氏距离损失（Euclidean Distance Loss）是对比学习中最简单的一种损失函数，用于学习样本对之间的相似性和差异性，优化模型的特征表示能力。
给定一对样本 $(x_i, x_j)$，欧氏距离损失的计算方式如下：
其中，$f(x_i)$ 和 $f(x_j)$ 分别表示样本 $x_i$ 和 $x_j$ 的特征表示，$| \cdot |2$ 表示欧氏距离，$y$ 表示样本对是否相似，$margin$ 表示样本对之间的距离阈值，控制相似样本之间的距离不超过一定范围，$L{euc}$ 表示欧氏距离损失。
欧氏距离损失的计算方式类似于 MSE（均方误差）损失，不同之处在于它只关注样本对之间的距离，而不是单个样本的预测值和真实值之间的距离。当样本对之间相似时，即 $y=1$ 时，欧氏距离损失主要考虑将它们的距离 $d$ 缩小到一定的范围内，从而提高它们之间的相似性；当样本对之间不相似时，即 $y=0$ 时，欧氏距离损失主要考虑增大它们的距离 $d$，使它们之间的差异性更加明显，从而提高模型的泛化能力。

## Triplet Loss
Triplet Loss要求输入包括三个样本，分别称为锚点（anchor）、正例（positive）和负例（negative）。锚点和正例是相似的，而锚点和负例是不相似的。模型的目标是将锚点和正例之间的距离最小化，同时将锚点和负例之间的距离最大化。
Triplet Loss的数学表达式为：
$L = \max(0, D(a,p) - D(a,n) + \alpha)$
其中，$D(a,p)$表示锚点$a$和正例$p$之间的距离，$D(a,n)$表示锚点$a$和负例$n$之间的距离，$\alpha$是一个称为margin的常数，用于控制相似对之间的最小距离。
Triplet Loss的目标是使相似对之间的距离尽可能小，同时使不相似对之间的距离尽可能大。在训练过程中，模型通过反向传播优化Triplet Loss，从而学习到良好的向量表示，可以用于匹配或对比任务。




